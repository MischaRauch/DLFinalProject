{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffaaf690-ea68-4452-9091-1b2d2eda981e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ViT in /usr/local/lib/python3.9/dist-packages (2.2.0)\n",
      "Requirement already satisfied: urwid>=2.1.2 in /usr/local/lib/python3.9/dist-packages (from ViT) (2.1.2)\n",
      "Requirement already satisfied: tasklib>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from ViT) (2.4.3)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.9/dist-packages (from tasklib>=2.4.3->ViT) (2022.1)\n",
      "Requirement already satisfied: tzlocal in /usr/local/lib/python3.9/dist-packages (from tasklib>=2.4.3->ViT) (4.2)\n",
      "Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.9/dist-packages (from tzlocal->tasklib>=2.4.3->ViT) (0.1.0.post0)\n",
      "Requirement already satisfied: tzdata in /usr/local/lib/python3.9/dist-packages (from pytz-deprecation-shim->tzlocal->tasklib>=2.4.3->ViT) (2022.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting vit-pytorch\n",
      "  Downloading vit_pytorch-0.37.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.10 in /usr/local/lib/python3.9/dist-packages (from vit-pytorch) (1.12.0+cu116)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from vit-pytorch) (0.13.0+cu116)\n",
      "Collecting einops>=0.4.1\n",
      "  Downloading einops-0.5.0-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.10->vit-pytorch) (4.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision->vit-pytorch) (2.28.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision->vit-pytorch) (1.23.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->vit-pytorch) (9.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->vit-pytorch) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision->vit-pytorch) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->vit-pytorch) (1.26.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision->vit-pytorch) (2.8)\n",
      "Installing collected packages: einops, vit-pytorch\n",
      "Successfully installed einops-0.5.0 vit-pytorch-0.37.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Imports here\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import \n",
    "import json\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision import transforms, models\n",
    "from torchvision.datasets import ImageFolder\n",
    "!pip install ViT\n",
    "!pip install vit-pytorch\n",
    "from vit_pytorch import ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51472309-72fe-4cc6-99a5-37584bad81b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/notebooks/flowers'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/valid'\n",
    "test_dir = data_dir + '/test'\n",
    "#!unzip /flowers.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85573f26-948e-41dd-b8d5-24f6262947dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your transforms for the training, validation, and testing sets\n",
    "#data_transforms = # 224 -> 256\n",
    "train_dir_transforms = transforms.Compose([transforms.RandomResizedCrop(224), # before all 225 ##256\n",
    "                                           transforms.RandomHorizontalFlip(),\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                                [0.229, 0.224, 0.225])])\n",
    "valid_dir_transforms = transforms.Compose([transforms.Resize(224),\n",
    "                                           transforms.CenterCrop(224),\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                                [0.229, 0.224, 0.225])])\n",
    "test_dir_transforms = transforms.Compose([transforms.Resize(224),\n",
    "                                          transforms.CenterCrop(224),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                               [0.229, 0.224, 0.225])])\n",
    "\n",
    "# TODO: Load the datasets with ImageFolder\n",
    "#image_datasets = \n",
    "train_data = ImageFolder(train_dir, train_dir_transforms)\n",
    "valid_set = ImageFolder(valid_dir, valid_dir_transforms)\n",
    "test_set = ImageFolder(test_dir, test_dir_transforms)\n",
    "\n",
    "# TODO: Using the image datasets and the trainforms, define the dataloaders\n",
    "#dataloaders = \n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size =64, shuffle=True)\n",
    "valid_data = torch.utils.data.DataLoader(valid_set, batch_size=64)\n",
    "test_data = torch.utils.data.DataLoader(test_set, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49285cf5-d714-4185-a589-5424cf9b2806",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cat_to_name.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ce89a34-11b5-4e5b-a7d5-394252a06ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.7240, -1.7240, -1.7069,  ..., -1.8953, -1.8953, -1.8610],\n",
       "          [-1.7583, -1.7412, -1.7412,  ..., -1.9124, -1.9124, -1.8953],\n",
       "          [-1.7925, -1.7754, -1.7754,  ..., -1.9295, -1.9295, -1.9124],\n",
       "          ...,\n",
       "          [-1.2274, -1.2617, -1.3130,  ..., -1.6042, -1.2959, -1.0219],\n",
       "          [-1.2274, -1.2959, -1.3473,  ..., -1.4329, -1.0390, -0.6794],\n",
       "          [-1.2445, -1.3130, -1.3473,  ..., -1.1932, -0.6965, -0.2856]],\n",
       " \n",
       "         [[-1.2129, -1.2129, -1.1954,  ..., -1.5630, -1.5630, -1.5980],\n",
       "          [-1.2654, -1.2479, -1.2479,  ..., -1.5980, -1.5980, -1.6331],\n",
       "          [-1.3179, -1.3004, -1.3004,  ..., -1.6331, -1.6331, -1.6506],\n",
       "          ...,\n",
       "          [-0.2500, -0.3025, -0.3725,  ..., -1.1078, -0.7577, -0.3901],\n",
       "          [-0.2325, -0.3025, -0.3901,  ..., -0.8978, -0.4951, -0.0399],\n",
       "          [-0.2675, -0.3375, -0.4251,  ..., -0.6352, -0.1099,  0.3627]],\n",
       " \n",
       "         [[-1.4036, -1.4036, -1.4210,  ..., -1.6302, -1.6302, -1.6127],\n",
       "          [-1.4384, -1.4210, -1.4384,  ..., -1.6650, -1.6650, -1.6476],\n",
       "          [-1.4559, -1.4559, -1.4559,  ..., -1.6999, -1.6999, -1.6824],\n",
       "          ...,\n",
       "          [-1.5779, -1.5604, -1.5779,  ..., -1.6476, -1.5430, -1.4210],\n",
       "          [-1.5953, -1.5953, -1.6127,  ..., -1.5081, -1.3687, -1.1596],\n",
       "          [-1.6302, -1.6302, -1.6302,  ..., -1.4036, -1.1247, -0.8458]]]),\n",
       " 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader.dataset[222]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a20d46a-ad4f-49a3-b21c-44672e915680",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a visual transformer, declaring the number of classes, image size, etc.\n",
    "Make sure that image_size is divisible by patch_size.\n",
    "\"\"\"\n",
    "v = ViT(\n",
    "    image_size = 224,\n",
    "    patch_size = 32, #32\n",
    "    num_classes = 102,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.5,\n",
    "#    emb_dropout = 0.1\n",
    ")\n",
    "model = ViT(\n",
    "    image_size = 224,\n",
    "    patch_size = 32, #32\n",
    "    num_classes = 102,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.5,\n",
    "#    emb_dropout = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f8a965d-b243-4014-9052-9ebcf8f12705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, valid_data, criterion):\n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "    for images, labels in valid_data:\n",
    "        \n",
    "        images,labels = images.to('cuda'), labels.to('cuda')\n",
    "        \n",
    "        output = model.forward(images)\n",
    "        batch_loss = criterion(output, labels)\n",
    "        test_loss += batch_loss.item()\n",
    "        \n",
    "        ps = torch.exp(output)\n",
    "        equality = (labels.data == ps.max(dim=1)[1])\n",
    "        accuracy += equality.type(torch.FloatTensor).mean()\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b055b74a-a7ec-4da7-a9d6-0dfb6eb485cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/12... Loss: -41.7776 Train Loss: -43.188..  Train Accuracy: 0.095\n",
      "Epoch: 2/12... Loss: -50.1237 Train Loss: -47.142..  Train Accuracy: 0.090\n",
      "Epoch: 3/12... Loss: -49.2122 Train Loss: -51.170..  Train Accuracy: 0.089\n",
      "Epoch: 4/12... Loss: -54.2257 Train Loss: -55.430..  Train Accuracy: 0.089\n",
      "Epoch: 5/12... Loss: -56.6199 Train Loss: -59.732..  Train Accuracy: 0.093\n",
      "Epoch: 6/12... Loss: -62.3951 Train Loss: -64.114..  Train Accuracy: 0.094\n",
      "Epoch: 7/12... Loss: -64.8251 Train Loss: -68.692..  Train Accuracy: 0.079\n",
      "Epoch: 8/12... Loss: -68.2049 Train Loss: -73.238..  Train Accuracy: 0.079\n",
      "Epoch: 9/12... Loss: -74.3376 Train Loss: -78.128..  Train Accuracy: 0.079\n",
      "Epoch: 10/12... Loss: -82.5395 Train Loss: -82.982..  Train Accuracy: 0.060\n",
      "Epoch: 11/12... Loss: -85.9194 Train Loss: -87.870..  Train Accuracy: 0.047\n",
      "Epoch: 12/12... Loss: -86.6795 Train Loss: -92.888..  Train Accuracy: 0.047\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n",
    "criterion = nn.functional.nll_loss\n",
    "epochs = 12\n",
    "#running_loss = 0\n",
    "model.to('cuda')\n",
    "for epoch in range(epochs):\n",
    "  model.train()\n",
    "  counter = 0\n",
    "  for data in trainloader:\n",
    "    X , y= data\n",
    "    X , y = X.to('cuda'), y.to('cuda')\n",
    "    optimizer.zero_grad() # clear gradient information.\n",
    "    #output = model(X) <- gives errors\n",
    "    output = model.forward(X)\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward() # do pack-propagation step\n",
    "    optimizer.step() # tell optimizer that you finished batch/iteration.\n",
    "    counter += 1\n",
    "    #running_loss += loss.item()\n",
    " \n",
    " \n",
    "  # network in eval mode for inference\n",
    "  model.eval()\n",
    "            \n",
    "  # Turn off gradients for validation, saves memory and computations\n",
    "  with torch.no_grad():\n",
    "      test_loss, accuracy = validation(model, valid_data, criterion)\n",
    "  print(\"Epoch: {}/{}...\".format(epoch+1,epochs),\n",
    "        \"Loss: {:.4f}\".format(loss.data),\n",
    "        \"Train Loss: {:.3f}.. \".format(test_loss/len(valid_data)),\n",
    "        \"Train Accuracy: {:.3f}\".format(accuracy/len(valid_data)))\n",
    "  running_loss = 0\n",
    "  model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a5ba0848-a619-46fc-842c-d6e6bca933ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.104\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained network.\n",
    "total = 0\n",
    "correct = 0\n",
    "model.to('cuda')\n",
    "with torch.no_grad():   # No need for keepnig track of necessary changes to the gradient.\n",
    "  for X,y in test_data:\n",
    "    X , y = X.to('cuda'), y.to('cuda')\n",
    "    output = model(X)\n",
    "    for idx, val in enumerate(output):\n",
    "      if torch.argmax(val) == y[idx]:\n",
    "        correct += 1\n",
    "      total += 1\n",
    "  print('Accuracy:', round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ab9858f-9f72-4ab1-8ed5-c822daa163b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.2461e+08, device='cuda:0')\n",
      "tensor(-1.2016e+08, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "optimizer = torch.optim.Adam(v.parameters(), lr=7.9)\n",
    "for epocs in range(2):\n",
    "  model.train()\n",
    "  counter = 0\n",
    "  for data in trainloader:\n",
    "    #print(f\"\\r This is: {counter} of total: {data_length} \",end = \" \")\n",
    "    X , y= data\n",
    "    X , y = X.to('cuda'), y.to('cuda')\n",
    "    optimizer.zero_grad() # clear gradient information.\n",
    "    output = model(X)\n",
    "    loss = nn.functional.nll_loss(output, y)\n",
    "    loss.backward() # do pack-propagation step\n",
    "    optimizer.step() # tell optimizer that you finished batch/iteration.\n",
    "    counter += 1\n",
    "  print(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f10fe5-676e-4c9d-91a3-9d7f694f03c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
